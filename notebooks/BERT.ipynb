{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT代码实现\n",
    "\n",
    "基于huggingface/Transformers库(3.1.0版本)中的pytorch版本BERT实现，我来实现自己的BERT模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 架构如下图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BertSelfAttention和BertSelfOutput构成了BertAttention，也就是Attention层的结构，再与全连接层BertIntermediate和BertOutput构成了一层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BERT架构图](./imgs/BERT.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from transformers import (BertConfig,\n",
    "                          BertTokenizer,\n",
    "                          set_seed,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保证每次跑的结果一致性\n",
    "set_seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这一部分是为了标准化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModelOutput:\n",
    "    \"\"\"\n",
    "    模型输出的基类，可能有hidden_states和attentions\n",
    "    last_hidden_state: 模型最后一层的输出，(batch_size, seq_length, hidden_size) eg. (64, 128, 768)\n",
    "    hidden_states: 元组，(num_hidden_layer+1)个，也就是13个，包含embedding的输出和其他所有层的输出\n",
    "    attentions: num_hidden_layer * (batch_size, num_heads, seq_length, seq_length) eg. 12 * (64, 12, 128, 128)\n",
    "    \"\"\"\n",
    "    \n",
    "    last_hidden_state: torch.FloatTensor\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModelOutputWithPooling:\n",
    "    \"\"\"\n",
    "    BERT模型输出的基类\n",
    "    last_hidden_state: 模型最后一层的输出，(batch_size, seq_length, hidden_size) eg. (64, 128, 768)\n",
    "    pooler_output: 模型最后一层的cls输出，乘以(hidden_size, hidden_size)后的结果，(batch_size, hidden_size) eg. (64, 768)\n",
    "    hidden_states: 元组，(num_hidden_layer+1)个，也就是13个，包含embedding的输出和其他所有层的输出\n",
    "    attentions: num_hidden_layer * (batch_size, num_heads, seq_length, seq_length) eg. 12 * (64, 12, 128, 128)\n",
    "    \"\"\"\n",
    "    \n",
    "    last_hidden_state: torch.FloatTensor\n",
    "    pooler_output: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BertForPreTrainingOutput:\n",
    "    \"\"\"\n",
    "    BertForPreTrainingModel的输出格式\n",
    "    \n",
    "    loss: 损失，一个值。包含MLM任务和NSP任务的交叉熵损失之和。\n",
    "    prediction_logits: MLM任务的logits，在softmax之前的结果。(batch_size, seq_length, hidden_size) eg. (64, 128, 768))\n",
    "    seq_relationship_logits: NSP任务的logits。(batch_size, 2), eg. (64, 2)\n",
    "    hidden_states: 元组，(num_hidden_layer+1)个，也就是13个，包含embedding的输出和其他所有层的输出\n",
    "    attentions: num_hidden_layer * (batch_size, num_heads, seq_length, seq_length) eg. 12 * (64, 12, 128, 128)\n",
    "    \"\"\"\n",
    "    \n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    prediction_logits: torch.FloatTensor = None\n",
    "    seq_relationship_logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MaskedLMOutput:\n",
    "    \"\"\"\n",
    "    masked language model输出的基类\n",
    "    \"\"\"\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NextSentencePredictionOutput:\n",
    "    \"\"\"\n",
    "    单NSP任务模型的输出\n",
    "    \"\"\"\n",
    "    \n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SequenceClassifierOutput:\n",
    "    \"\"\"\n",
    "    句子分类模型的输出基类\n",
    "    \"\"\"\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TokenClassifierOutput:\n",
    "    \"\"\"\n",
    "    token classification模型输出的基类\n",
    "    \"\"\"\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT模型的构建\n",
    "\n",
    "BERT模型构建，包含三层：embedding、encoder、pooler。其中，encoder包含多层layer。每个layer包含attention、全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer norm层\n",
    "BertLayerNorm = nn.LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding层实现，三个输入相加，然后layernorm，再然后dropout\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\" embeddings相关处理 \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 首先是词，位置，type三个相加\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        # LayerNorm & dropout\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        # 位置编码 (1, seq_length)\n",
    "        self.register_buffer('position_ids', torch.arange(config.max_position_embeddings).expand(1, -1))\n",
    "        \n",
    "    def forward(self, input_ids, position_ids=None, token_type_ids=None, inputs_embeds=None):\n",
    "        \"\"\"\n",
    "        input_ids (batch, seq_length)\n",
    "        position_ids (batch, seq_length) or None\n",
    "        type_ids (batch,) or None\n",
    "        \"\"\"\n",
    "        # 得到batch size和seq length\n",
    "        # 用于处理postion_ids和type_ids为空时的默认值\n",
    "        if inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.shape[:-1] \n",
    "        else:\n",
    "            input_shape = input_ids.shape \n",
    "        batch_size, seq_length = input_shape\n",
    "        \n",
    "        # 位置编码处理, 默认0-n\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "        # type编码处理，默认全是0，全是第一个\n",
    "        # 生成全是0的tensor的方法\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "        \n",
    "        # 词embedding优先使用输入的，其次是input_ids得到的\n",
    "        if inputs_embeds is not None:\n",
    "            word_embeds = inputs_embeds\n",
    "        else:\n",
    "            word_embeds = self.word_embeddings(input_ids)\n",
    "        # 位置和type，使用embedding查找\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.type_embeddings(token_type_ids)\n",
    "        \n",
    "        # 相加得到需要的结果，然后在经过layer norm层和dropout层\n",
    "        embeddings = word_embeds + position_embeds + token_type_embeds\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    self attention实现\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, 'embedding_size'):\n",
    "            raise ValueError(\n",
    "                f'The hidden size {config.hidden_size} is not a multiple of the number of attention '\n",
    "                f'heads (config.num_attention_heads)'\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # eg. query (768, 768)\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "    \n",
    "    def transpose_for_scores(self, x):\n",
    "        # x (batch_size, seq_length, hidden_states) eg. (64, 128, 768)\n",
    "        # new_x_shape (batch_size, seq_length, num_attention_heads, attention_head_size) eg. (64, 128, 12, 64)\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        # 转换shape；调整顺序\n",
    "        x = x.view(*new_x_shape)\n",
    "        # 转换后： (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64) \n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        return x\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # hidden_states (64, 128, 768)\n",
    "        # query (768, 768)\n",
    "        # mixed_query_layer (64, 128, 768)\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        # query_layer (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64) \n",
    "        # key_layer & value_layer同理\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # quey_layer\n",
    "        # (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64) \n",
    "        # key_layer.transpose(-1, -2):\n",
    "        # (batch_size, num_attention_heads, attention_head_size, seq_length) eg. (64, 12, 64, 128) \n",
    "        # 乘积结果 (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        # 除以attention_head_size的开根号 也就是除以12的开根号\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # attention_mask: (batch_size, num_heads, from_seq_length, to_seq_length)\n",
    "        # 但是设置的是哦户num_heads, from_seq_length设置为1，所以eg: (64, 1, 1, 128)\n",
    "        if attention_mask is not None:\n",
    "            # attention-scores: (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # 归一化attention_scores为概率率\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # attention_probs: (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "        # value_layer: (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64) \n",
    "        # context_layer: (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # context_layer: (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64)\n",
    "        # context_layer 新: (batch_size, seq_length, num_attention_heads, attention_head_size) eg. (64, 128, 12, 64)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        # new_context_layer_shape: (batch_size, seq_length, all_head_size) eg. (64, 128, 768)\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size, )\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        # context_layer: (batch_size, seq_length, all_head_size) eg. (64, 128, 768)\n",
    "        # attention_probs: (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    实现了attention之后的全连接和残差层\n",
    "    与BertSelfAttention一起构成了BERT的Attention层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        # hidden_states: (batch_size, seq_length, hidden_size) eg. (64, 128, 768)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bert Attention层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # 首先是self attention层\n",
    "        self_outputs = self.self(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        # 然后是全连接层和残差层\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        # 返回attention的输出，如果output_attentions为True的话，self_outptus第二位有attention权重\n",
    "        # 也就是要么一个结果，要么两个结果\n",
    "        outputs = (attention_output,) + self_outputs[1:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    \"\"\"\n",
    "    中间层，attention上面的全连接层的一半\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str) and config.hidden_act == 'gelu':\n",
    "            self.intermediate_act_fn = F.gelu\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "            \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    中间层，attention上面的全连接的另一半\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    bert的encoder中的一层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        # 第一个输出永远是attention output\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        # self_attention_outputs \n",
    "        # 如果output_attentions为False，则只有第一个输出\n",
    "        # 如果output_attentions为True，则有两个输出，第二个输出为attention的概率\n",
    "        outputs = self_attention_outputs[1:]\n",
    "        \n",
    "        # 下面全连接层\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        # layer_output, 是hidden_states\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        \n",
    "        # 打包输出\n",
    "        # (hidden_states, attention_probs) or (layer_outputs)\n",
    "        # hidden_states (batch_size, seq_length, hidden_size) eg (64, 128, 768)\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    bert encoder, bert模型分为三块，一个块是embedding，一块是encoder，一块是pooler\n",
    "    encoder包含12层的layer，每个layer又包含上下两层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # 保存多个layer的方法，nn.ModuleList\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=False,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "            \n",
    "            # 如果output_attentions == True 返回(hidden_states, attention_probs)\n",
    "            # 如果output_attentions == False, 返回(hidden_states)\n",
    "            # hidden_states (batch_size, seq_length, hidden_size) eg (64, 128, 768)\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "        \n",
    "        # hidden_states: (batch_size, seq_length, hidden_size) eg (64, 128, 768) 最后一层的hidden_states\n",
    "        # all_hidden_states: (num_hidden_layers+1) * hidden_states = 13 * (64, 128, 768)\n",
    "        # all_attentions: (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "        # 默认肯定返回hidden_states，另两个根据参数控制决定是否输出\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_attentions,\n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    \"\"\"\n",
    "    bert三个模块：embedding，encoder，pooler。这里是pooler模块， 将cls对应的hidden_states乘以(hidden_size, hidden_size)\n",
    "    也就是乘以 eg. (768, 768)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainedModel(nn.Module):\n",
    "    \"\"\"\n",
    "    bert最基本的类，实现的功能是通用的功能，也就是初始化权重等通用性功能。\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        初始化权重\n",
    "        \"\"\"\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        初始化权重\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        # BertLayerNorm的初始化？有什么权重？\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    bert模型，包含embedding、encoder、pooler三层；其中encoder包含多层layer，每个layer又分为attention层和全连接层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        \n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        # 四个输入相关的参数\n",
    "        # inputs_embeds与input_ids是二选一的关系\n",
    "        # input_ids + attention_mask + token_type_ids\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        # 三个输出相关的参数\n",
    "        # 是否输出attention；\n",
    "        # 是否输出hidden states\n",
    "        # 返回字典格式，或者dataclass格式\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        # 返回格式处理\n",
    "        # eg: output_attentions False; \n",
    "        # eg: output_hidden_states: False;\n",
    "        # eg: return_dict: False;\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # 四个输入相关的标准化处理\n",
    "        # input shape、device的处理；异常的警告\n",
    "        # input_shape: (batch_size, seq_length) eg. (64, 128)\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            # value error警告用法\n",
    "            raise ValueError(\"You cnanot sepcify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError('You have to specify either input_ids or inputs_embeds')\n",
    "        # device\n",
    "        device = input_ids.device if input_ids is not None  else inputs_embeds.device\n",
    "        # 处理attention_mask、token_type_ids\n",
    "        # attention_mask: (batch_size, seq_length) eg. (64, 128)\n",
    "        # token_type_ids: (batch_size, seq_length) eg. (64, 128)\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "        \n",
    "        # 获取extended_attention_mask (batch_size, num_heads, from_seq_length, to_seq_length)\n",
    "        # attention_mask: (batch_size, seq_length), input_shap: (batch_size, seq_length), devcie:\n",
    "        # attention_mask查看维度为3； .dim()\n",
    "        # 此时为 (batch_size, from_seq_length, to_seq_length)\n",
    "        # extended_attention_mask eg: (64, 1, 1, 128)\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'wrong shape for input_ids (shape {input_shape})'\n",
    "                f'or attention_mask (shape {attention_mask.shape})'\n",
    "            )\n",
    "        # 1.0 表示未mask，0.0表示mask\n",
    "        # 这里处理与之前有点不一样，是将未mak的attention置为0，mask的置为-10000\n",
    "        # 然后与softmax之前的值相加，结果是一样\n",
    "        extended_attention_mask = extended_attention_mask.to(device)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        # 处理完上面以后，开始真正进入bert了\n",
    "        # bert可以分成三段：分别是embedding阶段、encoder阶段、pooler阶段\n",
    "        # 下面这个是embedding阶段\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "        # 第二阶段 encoder阶段\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # 第三阶段 pooler阶段\n",
    "        sequence_output = encoder_outputs[0] if not return_dict else encoder_outputs.last_hidden_state\n",
    "        pooler_output = self.pooler(sequence_output)\n",
    "        \n",
    "        # 打包返回\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooler_output) + encoder_outputs[1:]\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooler_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT的各种head\n",
    "\n",
    "基于上面的BertModel，上面接入各种head，可以完成MLM、NSP的任务，可以完成分类、NER等任务。\n",
    "\n",
    "这里仅仅是head，并不是完整的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOnlyMLMHead(nn.Module):\n",
    "    \"\"\"\n",
    "    一层MLM的head，与bert基本模型配合使用\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # dense层和layernorm层\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str) and config.hidden_act == 'gelu':\n",
    "            self.transform_act_fn = F.gelu\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        # 解码层\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # dense层和layernorm层\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        # 解码层\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOnlyNSPHead(nn.Module):\n",
    "    \"\"\"\n",
    "    一层NSP的head，与bert基本模型配合使用\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "    \n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Module):\n",
    "    \"\"\"\n",
    "    一层预训练head，包含MLM与NSP任务\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertOnlyMLMHead(config)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "    \n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 接入head的各种bert模型\n",
    "\n",
    "MLM、NSP、预训练、分类、NER等任务的bert模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMaskedLM(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    MLM任务\n",
    "    不含labels输入，会预测\n",
    "    含labels输入，会返回\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        # last_hidden_state: 模型最后一层的输出，(batch_size, seq_length, hidden_size) eg. (64, 128, 768)\n",
    "        if return_dict:\n",
    "            sequence_output = outputs.last_hidden_state\n",
    "        else:\n",
    "            sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "        \n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            # -100 index = padding token\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # labels (batch_size, seq_length) eg. (64, 128)\n",
    "            # labels.view(-1) (batch_size*seq_length) eg. (64*128) = (8192) \n",
    "            # prediction_scores (batch_size, seq_length, vocab_size) eg (64, 128, 28996)\n",
    "            # prediction_scores.view(-1, 2) (batch_size*seq_lenght, vocab_size) eg: (8192, 28996)\n",
    "            # 交叉熵，将这种格式输入即可。也就是左边是行数，每行有28996个选项的概率，右边是对应的行数，每个是对应的答案。\n",
    "            # 可以忽视-100的label。\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "        \n",
    "        if not return_dict:\n",
    "            # \n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "        \n",
    "        return MaskedLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForNextSentencePrediction(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    NSP任务\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertOnlyNSPHead(config)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        # 输入\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        # 标签\n",
    "        next_sentence_label=None,\n",
    "        # 输出格式\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "#         pdb.set_trace()\n",
    "        if return_dict:\n",
    "            pooled_output = outputs.pooler_output\n",
    "        else:\n",
    "            pooled_output = outputs[1]\n",
    "        seq_relationship_scores = self.cls(pooled_output)\n",
    "        \n",
    "        next_sentence_loss = None\n",
    "        if next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), next_sentence_label.view(-1))\n",
    "            \n",
    "        if return_dict:\n",
    "            return NextSentencePredictionOutput(\n",
    "                loss=next_sentence_loss,\n",
    "                logits=seq_relationship_scores,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )        \n",
    "        else:\n",
    "            output = (seq_relationship_scores,) + outputs[2:]\n",
    "            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPreTraining(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    用于预训练任务，包含MLM任务和NSP任务\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.cls = BertPreTrainingHeads(config)\n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        # 两个预训练任务的labels； 第一个是MLM的，第二个是NSP的\n",
    "        labels=None,\n",
    "        next_sentence_label=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        bert_outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        # bert_output: last_hidden_state; pooler_output; hidden_states; attentions\n",
    "        # 四个结果中，前两个是必有；后两个根据参数决定是否有\n",
    "        if return_dict:\n",
    "            sequence_output, pooled_output = bert_outputs.last_hidden_state, bert_outputs.pooler_output\n",
    "        else:\n",
    "            sequence_output, pooled_output = bert_outputs[:2]\n",
    "\n",
    "        # 得到两个预训练任务的结果\n",
    "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
    "\n",
    "        total_loss = None\n",
    "        if labels is not None and next_sentence_label is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # labels (batch_size, seq_length) eg. (64, 128)\n",
    "            # labels.view(-1) (batch_size*seq_length) eg. (64*128) = (8192) \n",
    "            # prediction_scores (batch_size, seq_length, vocab_size) eg (64, 128, 28996)\n",
    "            # prediction_scores.view(-1, 2) (batch_size*seq_lenght, vocab_size) eg: (8192, 28996)\n",
    "            # 交叉熵，将这种格式输入即可。也就是左边是行数，每行有28996个选项的概率，右边是对应的行数，每个是对应的答案。\n",
    "            # 可以忽视-100的label。\n",
    "            masked_lm_loss = loss_fct(prediction_scores.view(-1, config.vocab_size), labels.view(-1))\n",
    "            # seq_relationship_score (batch_size, 2) eg. (64, 2)\n",
    "            # next_sentence_label (batch_size), eg. (64)\n",
    "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
    "            total_loss = masked_lm_loss + next_sentence_loss\n",
    "        \n",
    "        # loss: 1个tensor的值, MLM和NSP的损失之和。\n",
    "        # prediction_logits: MLM任务得到的logits (batch_size, seq_length, vocab_size) eg. (64, 128, 28996)\n",
    "        # seq_relationship_logits: NSP任务得到的logits (batch_size, 2) eg. (64, 2)\n",
    "        # hidden_states: 元组，(num_hidden_layer+1)个，也就是13个，包含embedding的输出和其他所有层的输出\n",
    "        # attentions: num_hidden_layer * (batch_size, num_heads, seq_length, seq_length) eg. 12 * (64, 12, 128, 128)\n",
    "        if return_dict:\n",
    "               return BertForPreTrainingOutput(\n",
    "                loss=total_loss,\n",
    "                prediction_logits=prediction_scores,\n",
    "                seq_relationship_logits=seq_relationship_score,\n",
    "                hidden_states=bert_outputs.hidden_states,\n",
    "                attentions=bert_outputs.attentions,\n",
    "            )         \n",
    "        else:\n",
    "            output = (prediction_scores, seq_relationship_score) + bert_outputs[2:]\n",
    "            return ((total_loss,) + output) if total_loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    用于文本分类任务\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        if return_dict:\n",
    "            pooled_output = outputs.pooler_output\n",
    "        else:\n",
    "            pooled_output = outputs[1]\n",
    "        # pooler_output: 模型最后一层的cls输出，乘以(hidden_size, hidden_size)后的结果，(batch_size, hidden_size) eg. (64, 768)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        # logits: (batch_size, num_labels) eg. (64, 280)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                # 做回归任务\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                # logits: (batch_size, num_labels) eg. (64, 280)\n",
    "                # labels: (batch_size,) eg. (64)\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForTokenClassification(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    用于序列标注任务（ner等）\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        if return_dict:\n",
    "            sequence_output = outputs.last_hidden_state\n",
    "        else:\n",
    "            sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        # logits: (batch_size, seq_length, num_labels) eg. (64, 128, 12)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        \n",
    "        loss = None\n",
    "        # labels (batch_size, seq_length) eg. (64, 128)\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # 仅仅关注未mask的数据\n",
    "            if attention_mask is not None:\n",
    "                # attention_mask (batch_size, seq_length) eg (64, 128)\n",
    "                # attention_mask.veiw(-1) (batch_size*seq_length) (64*128) = (8192)\n",
    "                # 关注的是1，为True；mask的为0，为False\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                # logits: (batch_size, seq_length, num_labels) eg. (64, 128, 12)\n",
    "                # logits.view(-1, self.num_labels) = (batch_size*seq_length, num_labels) = (8192, 12)\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                # torch.where(condition, x, y), 如果condition某个位置为True，则采用x对应位置的值，否则采用y对应位置的值\n",
    "                # 这里，就是关注的使用正确的label，不关注的使用-100，也就是交叉熵忽略的值\n",
    "                # type_as返回同labels相同的类型，也就是FloatTensor类型。\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "            else:\n",
    "                # 不同于分类，ner任务的标签个数肯定大于1，最常见的是B I O结构\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        if return_dict:\n",
    "            return TokenClassifierOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "        else:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 引入transformers中的Config和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = r'E:\\models\\huggingface\\bert-base-cased'\n",
    "# 12个label\n",
    "# pdb.set_trace()\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path,\n",
    "                                    num_labels=6,)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path,\n",
    "                                         config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试我们的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[1, 5, 3, 1], [2, 1, 2, 1]])\n",
    "position_ids = torch.tensor([[0, 1, 2, 3], [0, 1, 2, 3]], dtype=torch.long)\n",
    "token_type_ids = torch.tensor([[0, 0, 0, 0], [0, 0, 0, 0]], dtype=torch.long)\n",
    "output_hidden_states = True,\n",
    "output_attentions = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 基本模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_dict: True, model_class: <class '__main__.BertModel'>\n",
      "tensor([[[-0.6173,  1.3926, -1.1268,  ..., -1.1984,  0.8287,  0.4786],\n",
      "         [ 0.1401,  0.0600, -1.2550,  ..., -0.8059,  1.0439,  0.3112],\n",
      "         [-0.4751,  1.1698, -1.6823,  ..., -0.1699,  0.6303,  1.2419],\n",
      "         [-0.5519,  0.6194, -1.7218,  ..., -0.9902,  0.5199,  0.5989]],\n",
      "\n",
      "        [[-1.2104,  0.9546,  0.0531,  ...,  0.1068, -1.0703,  0.5158],\n",
      "         [-0.1883, -0.1162, -1.1842,  ..., -0.6901,  0.0073, -0.6441],\n",
      "         [-0.6517,  0.7814,  0.7595,  ...,  0.1150,  0.1251,  0.0587],\n",
      "         [-1.1482,  0.7543, -0.4053,  ..., -1.6291, -0.4497,  0.2858]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "return_dict: False, model_class: <class '__main__.BertModel'>\n",
      "tensor([[[-0.6173,  1.3926, -1.1268,  ..., -1.1984,  0.8287,  0.4786],\n",
      "         [ 0.1401,  0.0600, -1.2550,  ..., -0.8059,  1.0439,  0.3112],\n",
      "         [-0.4751,  1.1698, -1.6823,  ..., -0.1699,  0.6303,  1.2419],\n",
      "         [-0.5519,  0.6194, -1.7218,  ..., -0.9902,  0.5199,  0.5989]],\n",
      "\n",
      "        [[-1.2104,  0.9546,  0.0531,  ...,  0.1068, -1.0703,  0.5158],\n",
      "         [-0.1883, -0.1162, -1.1842,  ..., -0.6901,  0.0073, -0.6441],\n",
      "         [-0.6517,  0.7814,  0.7595,  ...,  0.1150,  0.1251,  0.0587],\n",
      "         [-1.1482,  0.7543, -0.4053,  ..., -1.6291, -0.4497,  0.2858]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "for return_dict in [True, False]:\n",
    "    set_seed(2020)\n",
    "    model_class = BertModel\n",
    "    print(f'return_dict: {return_dict}, model_class: {model_class}')\n",
    "    model = model_class(config)\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    if return_dict:\n",
    "        print(output.last_hidden_state)\n",
    "    else:\n",
    "        print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 预训练MLM和NSP的合成模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_dict: True, model_class: <class '__main__.BertForPreTraining'>\n",
      "tensor(11.1472, grad_fn=<AddBackward0>)\n",
      "return_dict: False, model_class: <class '__main__.BertForPreTraining'>\n",
      "tensor(11.1472, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor([[1, 5, 3, 1], [2, 1, 2, 1]])\n",
    "next_sentence_label = torch.tensor([0, 0])\n",
    "\n",
    "for return_dict in [True, False]:\n",
    "    set_seed(2020)\n",
    "    model_class = BertForPreTraining\n",
    "    print(f'return_dict: {return_dict}, model_class: {model_class}')\n",
    "    model = model_class(config)\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=position_ids,\n",
    "        token_type_ids=token_type_ids,\n",
    "        labels=labels,\n",
    "        next_sentence_label=next_sentence_label,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        output_attentions=output_attentions,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    if return_dict:\n",
    "        print(output.loss)\n",
    "    else:\n",
    "        print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. MLM的模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_dict: True, model_class: <class '__main__.BertForMaskedLM'>\n",
      "tensor(9.9528, grad_fn=<NllLossBackward>)\n",
      "return_dict: False, model_class: <class '__main__.BertForMaskedLM'>\n",
      "tensor(9.9528, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor([[1, 5, 3, 1], [2, 1, 2, 1]])\n",
    "\n",
    "for return_dict in [True, False]:\n",
    "    set_seed(2020)\n",
    "    model_class = BertForMaskedLM\n",
    "    print(f'return_dict: {return_dict}, model_class: {model_class}')\n",
    "    model = model_class(config)\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=position_ids,\n",
    "        token_type_ids=token_type_ids,\n",
    "        labels=labels,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        output_attentions=output_attentions,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    if return_dict:\n",
    "        print(output.loss)\n",
    "    else:\n",
    "        print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. NSP的模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_dict: True, model_class: <class '__main__.BertForNextSentencePrediction'>\n",
      "tensor(0.7778, grad_fn=<NllLossBackward>)\n",
      "return_dict: False, model_class: <class '__main__.BertForNextSentencePrediction'>\n",
      "tensor(0.7778, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "next_sentence_label = torch.tensor([0, 0])\n",
    "\n",
    "for return_dict in [True, False]:\n",
    "    set_seed(2020)\n",
    "    model_class = BertForNextSentencePrediction\n",
    "    print(f'return_dict: {return_dict}, model_class: {model_class}')\n",
    "    model = model_class(config)\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=position_ids,\n",
    "        token_type_ids=token_type_ids,\n",
    "        next_sentence_label=next_sentence_label,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        output_attentions=output_attentions,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    if return_dict:\n",
    "        print(output.loss)\n",
    "    else:\n",
    "        print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. 分类模型的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_dict: True, model_class: <class '__main__.BertForSequenceClassification'>\n",
      "tensor(2.5378, grad_fn=<NllLossBackward>)\n",
      "return_dict: False, model_class: <class '__main__.BertForSequenceClassification'>\n",
      "tensor(2.5378, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor([3, 1])\n",
    "config.num_labels = 12\n",
    "\n",
    "for return_dict in [True, False]:\n",
    "    set_seed(2020)\n",
    "    model_class = BertForSequenceClassification\n",
    "    print(f'return_dict: {return_dict}, model_class: {model_class}')\n",
    "    model = model_class(config)\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=position_ids,\n",
    "        token_type_ids=token_type_ids,\n",
    "        labels=labels,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        output_attentions=output_attentions,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    if return_dict:\n",
    "        print(output.loss)\n",
    "    else:\n",
    "        print(output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. NER模型的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_dict: True, model_class: <class '__main__.BertForTokenClassification'>\n",
      "tensor(2.4908, grad_fn=<NllLossBackward>)\n",
      "return_dict: False, model_class: <class '__main__.BertForTokenClassification'>\n",
      "tensor(2.4908, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor([[1, 2, 0, 0], [0, 0, 1, 2]])\n",
    "config.num_labels = 12\n",
    "\n",
    "for return_dict in [True, False]:\n",
    "    set_seed(2020)\n",
    "    model_class = BertForTokenClassification\n",
    "    print(f'return_dict: {return_dict}, model_class: {model_class}')\n",
    "    model = model_class(config)\n",
    "    output = model(\n",
    "        input_ids=input_ids,\n",
    "        position_ids=position_ids,\n",
    "        token_type_ids=token_type_ids,\n",
    "        labels=labels,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        output_attentions=output_attentions,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    if return_dict:\n",
    "        print(output.loss)\n",
    "    else:\n",
    "        print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
