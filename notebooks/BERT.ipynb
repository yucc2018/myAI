{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT代码实现\n",
    "\n",
    "基于huggingface/Transformers库(3.1.0版本)中的pytorch版本BERT实现，我来实现自己的BERT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import (BertConfig,\n",
    "                          BertTokenizer,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 首先引入transformers中的Config和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = '/dfsdata2/yucc1_data/models/huggingface/bert-base-cased'\n",
    "# 12个label\n",
    "# pdb.set_trace()\n",
    "config = BertConfig.from_pretrained(pretrained_model_name_or_path,\n",
    "                                    num_labels=6,)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path,\n",
    "                                         config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这一部分是为了标准化输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModelOutput:\n",
    "    \"\"\"\n",
    "    模型输出的基类，可能有hidden_states和attentions\n",
    "    last_hidden_state: 模型最后一层的输出，(batch_size, seq_length, hidden_size) eg. (64, 128, 768)\n",
    "    hidden_states: 元组，(num_hidden_layer+1)个，也就是13个，包含embedding的输出和其他所有层的输出\n",
    "    attentions: num_hidden_layer * (batch_size, num_heads, seq_length, seq_length) eg. 12 * (64, 12, 128, 128)\n",
    "    \"\"\"\n",
    "    \n",
    "    last_hidden_state: torch.FloatTensor\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaseModelOutputWithPooling:\n",
    "    \"\"\"\n",
    "    BERT模型输出的基类\n",
    "    last_hidden_state: 模型最后一层的输出，(batch_size, seq_length, hidden_size) eg. (64, 128, 768)\n",
    "    pooler_output: 模型最后一层的cls输出，乘以(hidden_size, hidden_size)后的结果，(batch_size, hidden_size) eg. (64, 768)\n",
    "    hidden_states: 元组，(num_hidden_layer+1)个，也就是13个，包含embedding的输出和其他所有层的输出\n",
    "    attentions: num_hidden_layer * (batch_size, num_heads, seq_length, seq_length) eg. 12 * (64, 12, 128, 128)\n",
    "    \"\"\"\n",
    "    \n",
    "    last_hidden_state: torch.FloatTensor\n",
    "    pooler_output: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT模型的构建\n",
    "\n",
    "BERT模型构建，包含三层：embedding、encoder、pooler。其中，encoder包含多层layer。每个layer包含attention、全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer norm层\n",
    "BertLayerNorm = nn.LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding层实现，三个输入相加，然后layernorm，再然后dropout\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\" embeddings相关处理 \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 首先是词，位置，type三个相加\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        # LayerNorm & dropout\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        # 位置编码 (1, seq_length)\n",
    "        self.register_buffer('position_ids', torch.arange(config.max_position_embeddings).expand(1, -1))\n",
    "        \n",
    "    def forward(self, input_ids, position_ids=None, token_type_ids=None, input_embeds=None):\n",
    "        \"\"\"\n",
    "        input_ids (batch, seq_length)\n",
    "        position_ids (batch, seq_length) or None\n",
    "        type_ids (batch,) or None\n",
    "        \"\"\"\n",
    "        # 得到batch size和seq length\n",
    "        # 用于处理postion_ids和type_ids为空时的默认值\n",
    "        if input_embeds is not None:\n",
    "            input_shape = input_embeds.shape[:-1] \n",
    "        else:\n",
    "            input_shape = input_ids.shape \n",
    "        batch_size, seq_length = input_shape\n",
    "        \n",
    "        # 位置编码处理, 默认0-n\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, :seq_length]\n",
    "        # type编码处理，默认全是0，全是第一个\n",
    "        # 生成全是0的tensor的方法\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
    "        \n",
    "        # 词embedding优先使用输入的，其次是input_ids得到的\n",
    "        if input_embeds is not None:\n",
    "            word_embeds = input_embeds\n",
    "        else:\n",
    "            word_embeds = self.word_embeddings(input_ids)\n",
    "        # 位置和type，使用embedding查找\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        token_type_embeds = self.type_embeddings(token_type_ids)\n",
    "        \n",
    "        # 相加得到需要的结果，然后在经过layer norm层和dropout层\n",
    "        embeddings = word_embeds + position_embeds + token_type_embeds\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    self attention实现\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, 'embedding_size'):\n",
    "            raise ValueError(\n",
    "                f'The hidden size {config.hidden_size} is not a multiple of the number of attention '\n",
    "                f'heads (config.num_attention_heads)'\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # eg. query (768, 768)\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "    \n",
    "    def transpose_for_scores(self, x):\n",
    "        # x (batch_size, seq_length, hidden_states) eg. (64, 128, 768)\n",
    "        # new_x_shape (batch_size, seq_length, num_attention_heads, attention_head_size) eg. (64, 128, 12, 64)\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        # 转换shape；调整顺序\n",
    "        x = x.view(*new_x_shape)\n",
    "        # 转换后： (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64) \n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        return x\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # 初始化\n",
    "        hidden_states = embedding_output\n",
    "        attention_mask = extended_attention_mask\n",
    "        output_attentions = output_attentions\n",
    "\n",
    "        # hidden_states (64, 128, 768)\n",
    "        # query (768, 768)\n",
    "        # mixed_query_layer (64, 128, 768)\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        # query_layer (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64) \n",
    "        # key_layer & value_layer同理\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # quey_layer\n",
    "        # (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64) \n",
    "        # key_layer.transpose(-1, -2):\n",
    "        # (batch_size, num_attention_heads, attention_head_size, seq_length) eg. (64, 12, 64, 128) \n",
    "        # 乘积结果 (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        # 除以attention_head_size的开根号 也就是除以12的开根号\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # attention_mask: (batch_size, num_heads, from_seq_length, to_seq_length)\n",
    "        # 但是设置的是哦户num_heads, from_seq_length设置为1，所以eg: (64, 1, 1, 128)\n",
    "        if attention_mask is not None:\n",
    "            # attention-scores: (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # 归一化attention_scores为概率率\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # attention_probs: (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "        # value_layer: (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64) \n",
    "        # context_layer: (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        # context_layer: (batch_size, num_attention_heads, seq_length, attention_head_size) eg. (64, 12, 128, 64)\n",
    "        # context_layer 新: (batch_size, seq_length, num_attention_heads, attention_head_size) eg. (64, 128, 12, 64)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        # new_context_layer_shape: (batch_size, seq_length, all_head_size) eg. (64, 128, 768)\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size, )\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        # context_layer: (batch_size, seq_length, all_head_size) eg. (64, 128, 768)\n",
    "        # attention_probs: (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    实现了attention之后的全连接和残差层\n",
    "    与BertSelfAttention一起构成了BERT的Attention层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        # hidden_states: (batch_size, seq_length, hidden_size) eg. (64, 128, 768)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Bert Attention层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # 首先是self attention层\n",
    "        self_outputs = self.self(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        # 然后是全连接层和残差层\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        # 返回attention的输出，如果output_attentions为True的话，self_outptus第二位有attention权重\n",
    "        # 也就是要么一个结果，要么两个结果\n",
    "        outputs = (attention_output,) + self_outputs[1:]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    \"\"\"\n",
    "    中间层，attention上面的全连接层的一半\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str) and config.hidden_act == 'gelu':\n",
    "            self.intermediate_act_fn = F.gelu\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "            \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    \"\"\"\n",
    "    中间层，attention上面的全连接的另一半\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "    \n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    bert的encoder中的一层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "        )\n",
    "        # 第一个输出永远是attention output\n",
    "        attention_output = self_attention_outputs[0]\n",
    "        # self_attention_outputs \n",
    "        # 如果output_attentions为False，则只有第一个输出\n",
    "        # 如果output_attentions为True，则有两个输出，第二个输出为attention的概率\n",
    "        outputs = self_attention_outputs[1:]\n",
    "        \n",
    "        # 下面全连接层\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        # layer_output, 是hidden_states\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        \n",
    "        # 打包输出\n",
    "        # (hidden_states, attention_probs) or (layer_outputs)\n",
    "        # hidden_states (batch_size, seq_length, hidden_size) eg (64, 128, 768)\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    bert encoder, bert模型分为三块，一个块是embedding，一块是encoder，一块是pooler\n",
    "    encoder包含12层的layer，每个layer又包含上下两层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # 保存多个layer的方法，nn.ModuleList\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=False,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "            \n",
    "            # 如果output_attentions == True 返回(hidden_states, attention_probs)\n",
    "            # 如果output_attentions == False, 返回(hidden_states)\n",
    "            # hidden_states (batch_size, seq_length, hidden_size) eg (64, 128, 768)\n",
    "            layer_outputs = layer_module(\n",
    "                hidden_states,\n",
    "                attention_mask=attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "        \n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "        \n",
    "        # hidden_states: (batch_size, seq_length, hidden_size) eg (64, 128, 768) 最后一层的hidden_states\n",
    "        # all_hidden_states: (num_hidden_layers+1) * hidden_states = 13 * (64, 128, 768)\n",
    "        # all_attentions: (batch_size, num_attention_heads, seq_length, seq_length) eg. (64, 12, 128, 128)\n",
    "        # 默认肯定返回hidden_states，另两个根据参数控制决定是否输出\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_attentions,\n",
    "        )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    \"\"\"\n",
    "    bert三个模块：embedding，encoder，pooler。这里是pooler模块， 将cls对应的hidden_states乘以(hidden_size, hidden_size)\n",
    "    也就是乘以 eg. (768, 768)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    \"\"\"\n",
    "    bert模型，包含embedding、encoder、pooler三层；其中encoder包含多层layer，每个layer又分为attention层和全连接层\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        # 四个输入相关的参数\n",
    "        # inputs_embeds与input_ids是二选一的关系\n",
    "        # input_ids + attention_mask + token_type_ids\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        input_embeds=None,\n",
    "        # 三个输出相关的参数\n",
    "        # 是否输出attention；\n",
    "        # 是否输出hidden states\n",
    "        # 返回字典格式，或者dataclass格式\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "#         pdb.set_trace()\n",
    "        # 返回格式处理\n",
    "        # eg: output_attentions False; \n",
    "        # eg: output_hidden_states: False;\n",
    "        # eg: return_dict: False;\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # 四个输入相关的标准化处理\n",
    "        # input shape、device的处理；异常的警告\n",
    "        # input_shape: (batch_size, seq_length) eg. (64, 128)\n",
    "        if input_ids is not None and input_embeds is not None:\n",
    "            # value error警告用法\n",
    "            raise ValueError(\"You cnanot sepcify both input_ids and input_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        elif input_embeds is not None:\n",
    "            input_shape = input_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError('You have to specify either input_ids or inputs_embeds')\n",
    "        # device\n",
    "        device = input_ids.device if input_ids is not None  else inputs_embeds.device\n",
    "        # 处理attention_mask、token_type_ids\n",
    "        # attention_mask: (batch_size, seq_length) eg. (64, 128)\n",
    "        # token_type_ids: (batch_size, seq_length) eg. (64, 128)\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(input_shape, device=device)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
    "        \n",
    "        # 获取extended_attention_mask (batch_size, num_heads, from_seq_length, to_seq_length)\n",
    "        # attention_mask: (batch_size, seq_length), input_shap: (batch_size, seq_length), devcie:\n",
    "        # attention_mask查看维度为3； .dim()\n",
    "        # 此时为 (batch_size, from_seq_length, to_seq_length)\n",
    "        # extended_attention_mask eg: (64, 1, 1, 128)\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f'wrong shape for input_ids (shape {input_shape})'\n",
    "                f'or attention_mask (shape {attention_mask.shape})'\n",
    "            )\n",
    "        # 1.0 表示未mask，0.0表示mask\n",
    "        # 这里处理与之前有点不一样，是将未mak的attention置为0，mask的置为-10000\n",
    "        # 然后与softmax之前的值相加，结果是一样\n",
    "        extended_attention_mask = extended_attention_mask.to(device)\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        # 处理完上面以后，开始真正进入bert了\n",
    "        # bert可以分成三段：分别是embedding阶段、encoder阶段、pooler阶段\n",
    "        # 下面这个是embedding阶段\n",
    "        embedding_output = self.embeddings(\n",
    "            input_ids=input_ids,\n",
    "            position_ids=position_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            input_embeds=input_embeds,\n",
    "        )\n",
    "        # 第二阶段 encoder阶段\n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # 第三阶段 pooler阶段\n",
    "        sequence_output = encoder_outputs[0] if not return_dict else encoder_outputs.last_hidden_state\n",
    "        pooler_output = self.pooler(sequence_output)\n",
    "        \n",
    "        # 打包返回\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "        return BaseModelOutputWithPooling(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooler_output,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        初始化权重\n",
    "        \"\"\"\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        初始化权重\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        # BertLayerNorm的初始化？有什么权重？\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT接入各种head\n",
    "\n",
    "基于上面的BertModel，上面接入各种head，可以完成MLM、NSP的任务，可以完成分类、NER等任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOnlyMLMHead(nn.Module):\n",
    "    \"\"\"\n",
    "    一层MLM的head，与bert基本模型配合使用\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # dense层和layernorm层\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str) and config.hidden_act == 'gelu':\n",
    "            self.transform_act_fn = F.gelu\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        # 解码层\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        # dense层和layernorm层\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        # 解码层\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOnlyNSPHead(nn.Module):\n",
    "    \"\"\"\n",
    "    一层NSP的head，与bert基本模型配合使用\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.seq_relationship = nn.Linear(cofnig.hidden_size, 2)\n",
    "    \n",
    "    def forward(self, pooled_output):\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPreTrainingHeads(nn.Module):\n",
    "    \"\"\"\n",
    "    一层预训练head，包含MLM与NSP任务\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertOnlyMLMHead(config)\n",
    "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
    "    \n",
    "    def forward(self, sequence_output, pooled_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMaskedLM(nn.Module):\n",
    "    \"\"\"\n",
    "    MLM任务\n",
    "    不含labels输入，会预测\n",
    "    含labels输入，会返回\n",
    "    \"\"\"\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForNextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    NSP任务\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPreTraining(nn.Module):\n",
    "    \"\"\"\n",
    "    用于预训练任务，包含MLM任务和NSP任务\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.predictions = Bert\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    用于文本分类任务\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForTokenClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    用于序列标注任务（ner等）\n",
    "    \"\"\"\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertModel(config)\n",
    "clss = BertPreTrainingHeads(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[1, 5, 3, 1], [2, 1, 2, 1]])\n",
    "position_ids = torch.tensor([[0, 1, 2, 3], [0, 1, 2, 3]], dtype=torch.long)\n",
    "token_type_ids = torch.tensor([[0, 0, 0, 0], [0, 0, 0, 0]], dtype=torch.long)\n",
    "\n",
    "return_dict = True\n",
    "\n",
    "bert_outputs = bert(\n",
    "    input_ids=input_ids,\n",
    "    position_ids=position_ids,\n",
    "    token_type_ids=token_type_ids,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    return_dict=return_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_output: last_hidden_state; pooler_output; hidden_states; attentions\n",
    "# 四个结果中，前两个是必有；后两个根据参数决定是否有\n",
    "if return_dict:\n",
    "    sequence_output, pooled_output = bert_outputs.last_hidden_state, bert_outputs.pooler_output\n",
    "else:\n",
    "    sequence_output, pooled_output = bert_outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到两个预训练任务的结果\n",
    "prediction_scores, seq_relationship_score = clss(sequence_output, pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([[1, 5, 3, 1], [2, 1, 2, 1]])\n",
    "next_sentence_label = torch.tensor([0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 5, 3, 1],\n",
       "        [2, 1, 2, 1]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 5, 3, 1, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = BertEncoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = BertLayer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x(\n",
    "    hidden_states=embedding_output,\n",
    "    attention_mask=extended_attention_mask,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    return_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.hidden_states[5].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 4, 4])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.attentions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(config.hidden_act, str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gelu'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.hidden_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = BertAttention(config)\n",
    "self_outputs = attention(\n",
    "    hidden_states=embedding_output,\n",
    "    attention_mask=extended_attention_mask,\n",
    "    output_attentions=output_attentions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 768])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
